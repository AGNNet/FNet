{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T06:46:41.381395Z",
     "start_time": "2021-06-19T06:46:40.710468Z"
    }
   },
   "source": [
    "# FNet\n",
    "\n",
    "- Paper: Deep Learning in Searching the Spectroscopic Redshift of Quasars\n",
    "\n",
    "- Authors: F. Rastegar Nia, M. T. Mirtorabi R. Moradi A. Vafaei.Sadr and Y.Wang\n",
    "\n",
    "\n",
    "### Reference\n",
    "- pipeline: [G2Net / efficientnet_b7 / baseline](https://www.kaggle.com/yasufuminakama/g2net-efficientnet-b7-baseline-training).\n",
    "- dataset: https://www.kaggle.com/ywangscience/sdss-iii-iv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import os\n",
    "import h5py\n",
    "import sys\n",
    "\n",
    "# ML related packages\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch.utils.data import data, Dataset, DataLoader, TensorDataset, ConcatDataset, random_split\n",
    "from torchvision import transforms\n",
    "#from sklearn.metrics import roc_auc_score\n",
    "#from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# plot\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    load_model = False\n",
    "    best_loss = 1e10\n",
    "    best_score = 1e10\n",
    "    debug = False\n",
    "    print_freq = 100\n",
    "    num_workers = 4\n",
    "    scheduler = \"ReduceLROnPlateau\" # CosineAnnealingLR / ReduceLROnPlateau / CosineAnnealingWarmRestarts\n",
    "    model_name = \"1dresnet-DR12Q-Redshift\"\n",
    "    epochs = 50\n",
    "    T_max = 5\n",
    "    lr = 0.2e-4\n",
    "    min_lr = 1e-7\n",
    "    batch_size = 128\n",
    "    val_batch_size = 100\n",
    "    weight_decay = 1e-5\n",
    "    gradient_accumulation_steps = 1\n",
    "    max_grad_norm = 1000 # Clipping Gradient Maximun\n",
    "    factor = 0.2\n",
    "    patience = 1\n",
    "    eps = 1e-7\n",
    "    seed = 1127802826\n",
    "    n_fold = 5\n",
    "    trn_fold = [0,1]  # [0, 1, 2, 3, 4]\n",
    "    target_col = \"target\"\n",
    "    train = True\n",
    "    SAVEDIR = Path('./')\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Utils\n",
    "# ====================================================\n",
    "def get_score(y_true, y_pred):\n",
    "    #score = roc_auc_score(y_true, y_pred)\n",
    "    score = np.mean(np.abs((y_pred-y_true)/(1+y_true)))\n",
    "    return score\n",
    "\n",
    "\n",
    "def init_logger(log_file=CFG.SAVEDIR / '1dresnet-DR12Q-Redshift-train.log'):\n",
    "    \"\"\"Save Log to file\"\"\"\n",
    "    from logging import getLogger, INFO, FileHandler,  Formatter,  StreamHandler\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=log_file)\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    logger.propagate = False\n",
    "    return logger\n",
    "\n",
    "LOGGER = init_logger()\n",
    "\n",
    "\n",
    "def seed_torch(seed=42):\n",
    "    \"\"\"Random seed\"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_torch(seed=CFG.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T06:46:41.392135Z",
     "start_time": "2021-06-19T06:46:41.383186Z"
    }
   },
   "outputs": [],
   "source": [
    "f = h5py.File('dr16q-log-zvi-quasar.h5', 'r') \n",
    "f.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(x_standard, x):\n",
    "    \"\"\"normalize redshift 0-1\"\"\"\n",
    "    x_norm = (x - np.min(x_standard))/(np.max(x_standard)-np.min(x_standard))\n",
    "    return x_norm\n",
    "\n",
    "def inv_norm(x_standard, x_norm):\n",
    "    x = x_norm*(np.max(x_standard)-np.min(x_standard)) + np.min(x_standard)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T06:46:41.641767Z",
     "start_time": "2021-06-19T06:46:41.402926Z"
    }
   },
   "outputs": [],
   "source": [
    "## read data\n",
    "FLUX_NORM = f['flux_norm'][:]\n",
    "Z_VI = f['z_vi'][:]\n",
    "Z_QN= f['z_qn'][:]\n",
    "Z_PIPE= f['z_pipe'][:]\n",
    "Z_PCA= f['z_pca'][:]\n",
    "Z_DR12Q= f['z_dr12q'][:]\n",
    "PLATE = f['plate'][:]\n",
    "MJD = f['mjd'][:]\n",
    "FIBERIS = f['fiberid'][:]\n",
    "\n",
    "\n",
    "## filter only DR12\n",
    "Z_DR12Q_12 = Z_DR12Q[Z_DR12Q>0]\n",
    "Z_PCA_12 = Z_PCA[Z_DR12Q>0]\n",
    "FLUX_NORM_12 = FLUX_NORM[Z_DR12Q>0]\n",
    "Z_VI_12 = Z_VI[Z_DR12Q>0]\n",
    "Z_QN_12 = Z_QN[Z_DR12Q>0]\n",
    "PLATE_12 = PLATE[Z_DR12Q>0]\n",
    "MJD_12 = MJD[Z_DR12Q>0]\n",
    "FIBERIS_12 = FIBERIS[Z_DR12Q>0]\n",
    "\n",
    "## filter only DR16, not in DR12\n",
    "Z_DR12Q_16 = Z_DR12Q[Z_DR12Q==-1]\n",
    "Z_PCA_16 = Z_PCA[Z_DR12Q==-1]\n",
    "FLUX_NORM_16 = FLUX_NORM[Z_DR12Q==-1]\n",
    "Z_VI_16 = Z_VI[Z_DR12Q==-1]\n",
    "Z_QN_16 = Z_QN[Z_DR12Q==-1]\n",
    "PLATE_16 = PLATE[Z_DR12Q==-1]\n",
    "MJD_16 = MJD[Z_DR12Q==-1]\n",
    "FIBERIS_16 = FIBERIS[Z_DR12Q==-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## split train and test data for original data\n",
    "\n",
    "# entire DR16\n",
    "features = torch.Tensor(FLUX_NORM).view(-1,1,4618)\n",
    "labels = torch.Tensor(norm(Z_VI_12,Z_VI)).view(-1,1)\n",
    "names = torch.IntTensor(list(zip(PLATE,MJD,FIBERIS)))\n",
    "\n",
    "features_train, features_test, labels_train, labels_test, names_train, names_test \\\n",
    "= train_test_split(features ,labels, names, test_size= 0.1,random_state = 42 )\n",
    "\n",
    "# only DR12\n",
    "features_12 = torch.Tensor(FLUX_NORM_12).view(-1,1,4618)\n",
    "labels_12 = torch.Tensor(norm(Z_VI_12,Z_VI_12)).view(-1,1)\n",
    "names_12 = torch.IntTensor(list(zip(PLATE_12,MJD_12,FIBERIS_12)))\n",
    "\n",
    "features_train_12, features_test_12, labels_train_12, labels_test_12, names_train_12, names_test_12 \\\n",
    "= train_test_split(features_12 ,labels_12, names_12, test_size= 0.1,random_state = 42 )\n",
    "\n",
    "#  only in DR16, not in DR12\n",
    "features_16 = torch.Tensor(FLUX_NORM_16).view(-1,1,4618)\n",
    "labels_16 = torch.Tensor(norm(Z_VI_12,Z_VI_16)).view(-1,1)\n",
    "names_16 = torch.IntTensor(list(zip(PLATE_16,MJD_16,FIBERIS_16)))\n",
    "\n",
    "features_train_16, features_test_16, labels_train_16, labels_test_16, names_train_16, names_test_16 \\\n",
    "= train_test_split(features_16 ,labels_16, names_16, test_size= 0.1,random_state = 42 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "def max_memory_allocated(device):\n",
    "    MB = 1024.0 * 1024.0\n",
    "    mem = torch.cuda.max_memory_allocated(device) / MB\n",
    "    return f\"{mem:.0f} MB\"\n",
    "\n",
    "def sdss_id(index):\n",
    "    return '-'.join(map(str, index))\n",
    "\n",
    "def get_result(result_df):\n",
    "    preds = result_df['preds'].values\n",
    "    labels = result_df[CFG.target_col].values\n",
    "    score = get_score(labels, preds)\n",
    "    LOGGER.info(f'Score: {score:<.9f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_loss(pred, label):\n",
    "    #return torch.mean(torch.abs((pred-label)/(1+label)))\n",
    "    return nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T06:46:44.331546Z",
     "start_time": "2021-06-19T06:46:44.324313Z"
    }
   },
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, use_1x1conv=False, stride=1):\n",
    "        super(Residual, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=500, padding=250, stride=stride)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=200, padding=100, stride=stride)\n",
    "        self.conv3 = nn.Conv1d(out_channels, out_channels, kernel_size=15, padding=6, stride=1)\n",
    "        if use_1x1conv:\n",
    "            self.conv4 = nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=stride)\n",
    "        else:\n",
    "            self.conv4 = None\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        self.bn3 = nn.BatchNorm1d(out_channels)\n",
    "        #self.fc = nn.Linear(in_features, out_features, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y1 = F.relu(self.bn1(self.conv1(x)))\n",
    "        y2 = F.relu(self.bn2(self.conv2(y1)))\n",
    "        y3 = self.bn3(self.conv3(y2))\n",
    "        if self.conv4:\n",
    "            x = self.conv4(x)\n",
    "        return F.relu(y3+x)\n",
    "    \n",
    "    \n",
    "def resnet_block(in_channels, out_channels, num_residuals, stride=1):\n",
    "    blk = []\n",
    "    for i in range(num_residuals):\n",
    "        if i==0:\n",
    "            blk.append(Residual(in_channels, out_channels, use_1x1conv=True, stride=stride))\n",
    "        else:\n",
    "            blk.append(Residual(out_channels, out_channels))\n",
    "    return nn.Sequential(*blk)\n",
    "\n",
    "def resnet():\n",
    "    model = nn.Sequential()\n",
    "    model.add_module('resnet_block_1', resnet_block(in_channels=1, out_channels=32, num_residuals=5, stride=1))\n",
    "    model.add_module('resnet_block_2', resnet_block(in_channels=32, out_channels=64, num_residuals=1, stride=1))\n",
    "    model.add_module('resnet_block_3', resnet_block(in_channels=64, out_channels=32, num_residuals=1, stride=1))\n",
    "    model.add_module('resnet_block_4', resnet_block(in_channels=32, out_channels=1, num_residuals=1, stride=1))\n",
    "    model.add_module('flatten', nn.Flatten())\n",
    "    model.add_module('fc1', nn.Sequential(nn.Linear(in_features=4618, out_features=796, bias=True), nn.ReLU()))\n",
    "    model.add_module('fc2', nn.Sequential(nn.Linear(in_features=796, out_features=199, bias=True), nn.ReLU()))\n",
    "    model.add_module('fc3', nn.Linear(in_features=199, out_features=1, bias=True))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(files, model, criterion, optimizer, epoch, scheduler, device):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    scores = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    start = end = time.time()\n",
    "    global_step = 0\n",
    "    \n",
    "    train_loader = DataLoader(files, \n",
    "                              batch_size=CFG.batch_size, \n",
    "                              shuffle=True, \n",
    "                              num_workers=CFG.num_workers)\n",
    "\n",
    "    for step, d in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        x = d[0].to(device)\n",
    "        labels = d[1].to(device)\n",
    "\n",
    "        batch_size = labels.size(0)\n",
    "        y_preds = model(x)\n",
    "        loss = criterion(y_preds, labels)\n",
    "        # record loss\n",
    "        losses.update(loss, batch_size)\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        loss.backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n",
    "        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0:\n",
    "            output = 'Epoch: [{0}/{1}][{2}/{3}] '\\\n",
    "                     'Loss: {loss.val:.9f}({loss.avg:.9f}) '\\\n",
    "                     'Grad: {grad_norm:.9f}  '\\\n",
    "                     'LR: {lr:.9f}  '\\\n",
    "                     'Elapsed: {remain:s} '\\\n",
    "                     'Max mem: {mem:s}'.format(\n",
    "                      epoch+1, CFG.epochs, step, len(train_loader),\n",
    "                      loss=losses,\n",
    "                      grad_norm=grad_norm,\n",
    "                      #lr=scheduler.get_last_lr()[0],\n",
    "                      lr=optimizer.param_groups[0]['lr'],\n",
    "                      remain=timeSince(start, float(step + 1) / len(train_loader)),\n",
    "                      mem=max_memory_allocated(CFG.device))\n",
    "            #print(output)\n",
    "            LOGGER.info(output)\n",
    "    return losses.avg\n",
    "\n",
    "\n",
    "def valid_fn(files, model, criterion, device):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    scores = AverageMeter()\n",
    "    # switch to evaluation mode\n",
    "    model.eval()\n",
    "    filenames = []\n",
    "    targets = []\n",
    "    preds = []\n",
    "    start = end = time.time()\n",
    "    \n",
    "    valid_loader = DataLoader(files, \n",
    "                              batch_size=CFG.batch_size, \n",
    "                              shuffle=True, \n",
    "                              num_workers=CFG.num_workers)\n",
    "    \n",
    "    for step, d in enumerate(valid_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        \n",
    "        targets.extend(d[1])\n",
    "        filenames.extend(d[2].numpy())\n",
    "        x = d[0].to(device)\n",
    "        labels = d[1].to(device)\n",
    "\n",
    "        batch_size = labels.size(0)\n",
    "        # compute loss\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(x)\n",
    "            \n",
    "        loss = criterion(y_preds.view(-1), labels.view(-1))\n",
    "        losses.update(loss.item(), batch_size)\n",
    "\n",
    "        preds.append(y_preds.to('cpu').numpy())\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0: \n",
    "            output = 'EVAL: [{0}/{1}] '\\\n",
    "                     'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\\\n",
    "                     'Elapsed {remain:s} '\\\n",
    "                     'Loss: {loss.val:.9f}({loss.avg:.9f}) '.format(\n",
    "                     step, len(valid_loader), batch_time=batch_time,\n",
    "                     data_time=data_time, loss=losses,\n",
    "                     remain=timeSince(start, float(step+1)/len(valid_loader)),\n",
    "                     )\n",
    "            LOGGER.info(output)\n",
    "    predictions = np.concatenate(preds).reshape(-1)\n",
    "    return losses.avg, predictions, np.array(targets), np.array(filenames).reshape(-1,3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Train loop\n",
    "# ====================================================\n",
    "def train_loop(train_files, val_files, fold=99, load_model = False):\n",
    "    \n",
    "    if (CFG.n_fold==1):\n",
    "        LOGGER.info(f\"========== training ==========\")\n",
    "    else:\n",
    "        LOGGER.info(f\"========== fold: {fold} training ==========\")\n",
    "    \n",
    "    # ====================================================\n",
    "    # scheduler \n",
    "    # ====================================================\n",
    "    def get_scheduler(optimizer):\n",
    "        if CFG.scheduler=='ReduceLROnPlateau':\n",
    "            scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n",
    "                                                             mode='min', \n",
    "                                                             factor=CFG.factor, \n",
    "                                                             patience=CFG.patience, \n",
    "                                                             verbose=True, \n",
    "                                                             eps=CFG.eps)\n",
    "        elif CFG.scheduler=='CosineAnnealingLR':\n",
    "            scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, \n",
    "                                                             T_max=CFG.T_max, \n",
    "                                                             eta_min=CFG.min_lr, \n",
    "                                                             last_epoch=-1)\n",
    "        elif CFG.scheduler=='CosineAnnealingWarmRestarts':\n",
    "            scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, \n",
    "                                                                       T_0=CFG.T_0, \n",
    "                                                                       T_mult=1, \n",
    "                                                                       eta_min=CFG.min_lr, \n",
    "                                                                       last_epoch=-1)\n",
    "        return scheduler\n",
    "\n",
    "    # ====================================================\n",
    "    # model & optimizer\n",
    "    # ====================================================\n",
    "#    if first_run:\n",
    "    model.to(CFG.device)\n",
    "    if load_model:\n",
    "        model.load_state_dict(torch.load(CFG.SAVEDIR / f\"{CFG.model_name}_fold{fold}_best_loss.pth\")[\"model\"])\n",
    "        LOGGER.info(f\"========== Model loaded: {CFG.model_name}_fold{fold}_best_score.pth ==========\")\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay)\n",
    "    scheduler = get_scheduler(optimizer)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    if load_model:\n",
    "        best_score = CFG.best_score\n",
    "        best_loss = CFG.best_loss\n",
    "    else:\n",
    "        best_score = np.inf\n",
    "        best_loss = np.inf\n",
    "\n",
    "\n",
    "    # ====================================================\n",
    "    # loop\n",
    "    # ====================================================\n",
    "    \n",
    "    for epoch in range(CFG.epochs):\n",
    "        print(\"\\n\\n\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # train\n",
    "        avg_loss = train_fn(train_files, model, criterion, optimizer, epoch, scheduler, CFG.device)\n",
    "\n",
    "        # eval\n",
    "        avg_val_loss, preds, targets, files = valid_fn(val_files, model, criterion, CFG.device)\n",
    "        files = np.array([sdss_id(x) for x in np.array(files).reshape(-1,3)])\n",
    "        valid_result_df = pd.DataFrame({\"target\": targets, \"preds\": preds, \"id\": files})\n",
    "        \n",
    "        # scheduler the learning rate\n",
    "        if isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            scheduler.step(avg_val_loss)\n",
    "        elif isinstance(scheduler, optim.lr_scheduler.CosineAnnealingLR):\n",
    "            scheduler.step()\n",
    "        elif isinstance(scheduler, optim.lr_scheduler.CosineAnnealingWarmRestarts):\n",
    "            scheduler.step()\n",
    "\n",
    "        # scoring\n",
    "        score = get_score(targets, preds)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.9f}  avg_val_loss: {avg_val_loss:.9f}  time: {elapsed:.0f}s')\n",
    "        LOGGER.info(f'Epoch {epoch+1} - Score: {score:.9f}')\n",
    "            \n",
    "        # save models of best score and best loss\n",
    "        if score < best_score:\n",
    "            best_score = score\n",
    "            LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.9f} Model')\n",
    "            torch.save({'model': model.state_dict(), \n",
    "                        'preds': preds},\n",
    "                        CFG.SAVEDIR / f'{CFG.model_name}_fold{fold}_best_score.pth')\n",
    "            \n",
    "        if avg_val_loss < best_loss:\n",
    "            best_loss = avg_val_loss\n",
    "            LOGGER.info(f'Epoch {epoch+1} - Save Best Loss: {best_loss:.9f} Model')\n",
    "            torch.save({'model': model.state_dict(), \n",
    "                        'preds': preds},\n",
    "                        CFG.SAVEDIR / f'{CFG.model_name}_fold{fold}_best_loss.pth')\n",
    "    \n",
    "    # prediction from the model of best loss\n",
    "    valid_result_df[\"preds\"] = torch.load(CFG.SAVEDIR / f\"{CFG.model_name}_fold{fold}_best_loss.pth\",\n",
    "                                          map_location=\"cpu\")[\"preds\"]\n",
    "\n",
    "    return valid_result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = TensorDataset(features_train_12[:], labels_train_12[:], names_train_12[:])\n",
    "test_files = TensorDataset(features_test_12[:], labels_test_12[:], names_test_12[:])\n",
    "all_files = TensorDataset(features_12[:], labels_12[:], names_12[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result(result_df):\n",
    "    preds = result_df['preds'].values\n",
    "    labels = result_df[CFG.target_col].values\n",
    "    score = get_score(labels, preds)\n",
    "    LOGGER.info(f'Score: {score:<.9f}')\n",
    "\n",
    "if CFG.train:\n",
    "    # train \n",
    "    LOGGER.info(f\"********** Start at {time.asctime( time.localtime(time.time()) )}  **********\")\n",
    "    \n",
    "    # training one time by setting CFG.n_fold=1\n",
    "    if (CFG.n_fold==1):\n",
    "        oof_df = train_loop(train_files, test_files, load_model=CFG.load_model)\n",
    "        LOGGER.info(f\"========== result ==========\")\n",
    "        get_result(oof_df)\n",
    "    \n",
    "    # K-Fold traning if CFG.n_fold>1\n",
    "    if (CFG.n_fold>1):\n",
    "        oof_df = pd.DataFrame()\n",
    "        kf = KFold(n_splits=CFG.n_fold, shuffle=True, random_state=CFG.seed)\n",
    "        folds = list(kf.split(all_files))\n",
    "        for fold in range(CFG.n_fold):\n",
    "            if fold in CFG.trn_fold:\n",
    "                trn_idx, val_idx = folds[fold]\n",
    "                train_files = TensorDataset(*all_files[trn_idx])\n",
    "                valid_files = TensorDataset(*all_files[val_idx])\n",
    "                _oof_df = train_loop(train_files, valid_files, fold, load_model=CFG.load_model)\n",
    "                oof_df = pd.concat([oof_df, _oof_df])\n",
    "                LOGGER.info(f\"========== fold: {fold} result ==========\")\n",
    "                get_result(_oof_df)\n",
    "        # CV result\n",
    "        LOGGER.info(f\"========== CV result ==========\")\n",
    "        get_result(oof_df)\n",
    "        \n",
    "    # save result\n",
    "    oof_df.to_csv(CFG.SAVEDIR / f\"{CFG.model_name}.csv\", index=False)\n",
    "    LOGGER.info(f\"********** End at {time.asctime( time.localtime(time.time()) )} **********\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 99\n",
    "\n",
    "def get_scheduler(optimizer):\n",
    "    if CFG.scheduler=='ReduceLROnPlateau':\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n",
    "                                                         mode='min', \n",
    "                                                         factor=CFG.factor, \n",
    "                                                         patience=CFG.patience, \n",
    "                                                         verbose=True, \n",
    "                                                         eps=CFG.eps)\n",
    "    elif CFG.scheduler=='CosineAnnealingLR':\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, \n",
    "                                                         T_max=CFG.T_max, \n",
    "                                                         eta_min=CFG.min_lr, \n",
    "                                                         last_epoch=-1)\n",
    "    elif CFG.scheduler=='CosineAnnealingWarmRestarts':\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, \n",
    "                                                                   T_0=CFG.T_0, \n",
    "                                                                   T_mult=1, \n",
    "                                                                   eta_min=CFG.min_lr, \n",
    "                                                                   last_epoch=-1)\n",
    "    return scheduler\n",
    "\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DR12 Sample, filtered from DR16Q by Z_DR12Q>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_12, b_12, c_12, d_12 = valid_fn(TensorDataset(features_test_12, labels_test_12, names_test_12), model, criterion, CFG.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_12 = 300000*((inv_norm(Z_VI_12, b_12) - inv_norm(Z_VI_12, c_12))/(1+inv_norm(Z_VI_12, c_12)))\n",
    "print(abs(e_12).mean(),e_12.mean(),np.median(abs(e_12)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(e_12[np.abs(e_12)<6000])/len(e_12),len(e_12[np.abs(e_12)<12000])/len(e_12),len(e_12[np.abs(e_12)<30000])/len(e_12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DR16 Sample, filtered from DR16Q by Z_DR12Q==-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_16, b_16, c_16, d_16 = valid_fn(TensorDataset(features_16, labels_16, names_16), model, criterion, CFG.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_16 = 300000*((inv_norm(Z_VI_12, b_16) - inv_norm(Z_VI_12, c_16))/(1+inv_norm(Z_VI_12, c_16)))\n",
    "print(abs(e_16).mean(),e_16.mean(),np.median(abs(e_16)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(e_16[np.abs(e_16)<6000])/len(e_16),len(e_16[np.abs(e_16)<12000])/len(e_16),len(e_16[np.abs(e_16)<30000])/len(e_16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QuasarNet\n",
    "e_Q_16 = 300000*((Z_QN_16 - Z_VI_16)/(1+ Z_VI_16))\n",
    "print(abs(e_Q_16).mean(),e_Q_16.mean(),np.median(abs(e_Q_16)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(e_Q_16[np.abs(e_Q_16)<6000])/len(e_Q_16),len(e_Q_16[np.abs(e_Q_16)<12000])/len(e_Q_16),len(e_Q_16[np.abs(e_Q_16)<30000])/len(e_Q_16))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
